# Toxic Comment Classification

Being anonymous over the internet can sometimes make people say nasty things that they normally would not in real life. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments. Current models don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content). The goal is to build a multi-headed model that’s capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate.

Warning: This article may contain some graphic and vulgar language and themes. 
